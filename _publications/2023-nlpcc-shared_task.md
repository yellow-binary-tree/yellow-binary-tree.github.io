---
title: "Overview of the NLPCC 2023 Shared Task 10: Learn to Watch TV: Multimodal Dialogue Understanding and Response Generation"
authors: "<b>Yueqian Wang</b>, Yuxuan Wang, Dongyan Zhao"
collection: publications
permalink: /publication/2023-nlpcc-shared_task
excerpt: 'Hosted a shared task about video dialogue understanding and prediction at NLPCC 2023.'
date: 2023-10-13
venue: 'Natural Language Processing and Chinese Computing'
paperurl: 'https://link.springer.com/chapter/10.1007/978-3-031-44699-3_37'
---

Abstract: Video-grounded dialogue understanding is a challenging problem that requires machine to perceive, parse and reason over situated semantics extracted from weakly aligned video and dialogues. Most existing benchmarks treat both modalities the same as a frame-independent visual understanding task, while neglecting the intrinsic attributes in multimodal dialogues, such as scene and topic transitions. In this paper, we present Video-grounded Scene&Topic AwaRe dialogue (VSTAR) dataset, a large scale video-grounded dialogue understanding dataset based on 395 TV series. Based on VSTAR, we propose two benchmarks for video-grounded dialogue understanding: scene segmentation and topic segmentation, and one benchmark for video-grounded dialogue generation. Comprehensive experiments are performed on these benchmarks to demonstrate the importance of multimodal information and segments in video-grounded dialogue understanding and generation.